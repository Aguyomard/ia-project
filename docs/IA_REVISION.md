# ğŸ“š RÃ©vision - IntÃ©gration IA avec Node.js

> RÃ©capitulatif technique de l'intÃ©gration de Mistral AI dans une application Node.js/Express avec Vue.js

---

## ğŸ“‹ Table des matiÃ¨res

1. [Architecture globale](#1-architecture-globale)
2. [MistralService - Le cÅ“ur de l'IA](#2-mistralservice---le-cÅ“ur-de-lia)
3. [Robustesse - Retry avec Exponential Backoff](#3-robustesse---retry-avec-exponential-backoff)
4. [Gestion du Contexte - Sliding Window](#4-gestion-du-contexte---sliding-window)
5. [Streaming avec SSE](#5-streaming-avec-sse)
6. [Persistance avec Prisma](#6-persistance-avec-prisma)
7. [Embeddings et Recherche Vectorielle](#7-embeddings-et-recherche-vectorielle)
8. [RAG - Retrieval-Augmented Generation](#8-rag---retrieval-augmented-generation)
9. [Query Rewriting - Reformulation de requÃªtes](#9-query-rewriting---reformulation-de-requÃªtes)
10. [Hybrid Search - Recherche hybride](#10-hybrid-search---recherche-hybride)
11. [Architecture Clean Architecture](#11-architecture-clean-architecture)
12. [Frontend Vue.js](#12-frontend-vuejs)
13. [Concepts clÃ©s Ã  retenir](#13-concepts-clÃ©s-Ã -retenir)

---

## 1. Architecture globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FRONTEND (Vue.js)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ChatBot    â”‚  â”‚  Documents  â”‚  â”‚  Vue Router             â”‚  â”‚
â”‚  â”‚  View       â”‚  â”‚  View (RAG) â”‚  â”‚  /chat, /documents      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ HTTP / SSE
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  BACKEND (Clean Architecture)                    â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ğŸ”´ INFRASTRUCTURE                                         â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ http/ (Routes, Controllers)                         â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ persistence/ (Repositories Prisma)                  â”‚   â”‚
â”‚  â”‚  â””â”€â”€ external/ (MistralClient)                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                   â”‚
â”‚                              â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ğŸ”µ APPLICATION                                            â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ usecases/ (StreamMessageUseCase, AddDocumentUseCase)â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ services/ (ConversationService, RAGService)         â”‚   â”‚
â”‚  â”‚  â””â”€â”€ ports/ (Interfaces IMistralClient, IRAGService)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                   â”‚
â”‚                              â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ğŸŸ¢ DOMAIN                                                 â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ entities/ (Conversation, Message, Document)         â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ valueObjects/ (MessageRole, Embedding, UUID)        â”‚   â”‚
â”‚  â”‚  â””â”€â”€ repositories/ (Interfaces IConversationRepository)  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MISTRAL AI API  â”‚  â”‚   POSTGRESQL     â”‚  â”‚ RERANK SERVICE   â”‚
â”‚  - Chat          â”‚  â”‚   - Prisma ORM   â”‚  â”‚ (Python/FastAPI) â”‚
â”‚  - Embeddings    â”‚  â”‚   - pgvector     â”‚  â”‚ - bge-reranker   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. MistralService - Le cÅ“ur de l'IA

### 2.1 Structure du service

```typescript
// src/services/mistral/MistralService.ts

export class MistralService {
  private readonly client: Mistral; // SDK Mistral
  private readonly defaultModel: string; // ex: 'mistral-tiny'
  private readonly defaultTemperature: number; // ex: 0.7
  private readonly retryOptions: RetryOptions; // Config retry

  constructor(config: MistralConfig = {}) {
    // Initialisation avec API key depuis env ou config
  }
}
```

### 2.2 MÃ©thodes principales

| MÃ©thode                             | Description           | Retour                  |
| ----------------------------------- | --------------------- | ----------------------- |
| `chat(message, options)`            | Message simple        | `Promise<string>`       |
| `chatJSON<T>(message, options)`     | RÃ©ponse JSON typÃ©e    | `Promise<T>`            |
| `complete(messages, options)`       | Conversation complÃ¨te | `Promise<string>`       |
| `streamComplete(messages, options)` | Streaming             | `AsyncIterable<string>` |
| `streamChat(message, options)`      | Streaming simple      | `AsyncIterable<string>` |

### 2.3 Options disponibles

```typescript
interface ChatOptions {
  model?: string; // ModÃ¨le Ã  utiliser
  systemPrompt?: string; // Prompt systÃ¨me
  temperature?: number; // CrÃ©ativitÃ© (0-1)
  maxTokens?: number; // Limite de tokens en sortie
  jsonMode?: boolean; // Forcer rÃ©ponse JSON
  autoTruncate?: boolean; // Sliding window auto
  reservedForResponse?: number; // Tokens rÃ©servÃ©s pour rÃ©ponse
}
```

### 2.4 Pattern Singleton

```typescript
// RÃ©cupÃ©rer l'instance unique
const mistral = getMistralService();

// Reset pour les tests
resetMistralService();
```

**Pourquoi ?** Ã‰vite de crÃ©er plusieurs clients, partage la configuration.

---

## 3. Robustesse - Retry avec Exponential Backoff

### 3.1 Le problÃ¨me

Les API d'IA sont **fragiles** :

- `429` Too Many Requests (rate limit)
- `500/502/503/504` Erreurs serveur
- `ECONNRESET` Connexion coupÃ©e

### 3.2 La solution : Exponential Backoff

```
Tentative 1 â†’ Ã‰chec (429)
    â†“ Attendre 1s
Tentative 2 â†’ Ã‰chec (503)
    â†“ Attendre 2s
Tentative 3 â†’ Ã‰chec (500)
    â†“ Attendre 4s
Tentative 4 â†’ SuccÃ¨s âœ…
```

### 3.3 ImplÃ©mentation

```typescript
// src/utils/retry.ts

export async function withRetry<T>(
  fn: () => Promise<T>,
  options: RetryOptions = {}
): Promise<T> {
  const { maxRetries = 3, baseDelay = 1000, multiplier = 2 } = options;

  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      if (attempt >= maxRetries || !shouldRetry(error)) {
        throw error;
      }
      const delay = baseDelay * Math.pow(multiplier, attempt);
      await sleep(delay);
    }
  }
}
```

### 3.4 Jitter (variation alÃ©atoire)

```typescript
// Ã‰vite le "thundering herd" (tous les clients retentent en mÃªme temps)
const jitter = exponentialDelay * 0.25 * (Math.random() * 2 - 1);
const finalDelay = exponentialDelay + jitter;
```

### 3.5 Configuration

| ParamÃ¨tre    | DÃ©faut  | Description              |
| ------------ | ------- | ------------------------ |
| `maxRetries` | 3       | Nombre max de tentatives |
| `baseDelay`  | 1000ms  | DÃ©lai initial            |
| `multiplier` | 2       | Facteur d'augmentation   |
| `maxDelay`   | 30000ms | DÃ©lai max (30s)          |

### 3.6 Erreurs retryables

```typescript
const RETRYABLE_STATUS_CODES = [429, 500, 502, 503, 504];
const RETRYABLE_NETWORK_ERRORS = ['ECONNRESET', 'ETIMEDOUT', 'ECONNREFUSED'];
```

---

## 4. Gestion du Contexte - Sliding Window

### 4.1 Le problÃ¨me

Chaque modÃ¨le a une **limite de contexte** :

| ModÃ¨le               | Limite         |
| -------------------- | -------------- |
| mistral-tiny         | 32 000 tokens  |
| mistral-large-latest | 128 000 tokens |
| GPT-4o               | 128 000 tokens |

Si dÃ©passÃ© â†’ Erreur `400 Bad Request - Context Length Exceeded`

### 4.2 La solution : Sliding Window

```
AVANT (50 messages, ~35000 tokens)
[System] Tu es un assistant...
[User] Message 1
[Assistant] RÃ©ponse 1
[User] Message 2
...
[User] Message 50        â† DÃ©passe la limite !

APRÃˆS (sliding window)
[System] Tu es un assistant...  â† TOUJOURS gardÃ©
[User] Message 35               â† Messages rÃ©cents
[Assistant] RÃ©ponse 35
...
[User] Message 50               â† OK, ~28000 tokens
```

### 4.3 Estimation des tokens

```typescript
// Approximation : 1 token â‰ˆ 3.5 caractÃ¨res
export function estimateTokens(text: string): number {
  const charCount = text.length;
  const tokenEstimate = Math.ceil(charCount / 3.5);
  return tokenEstimate + 4; // +4 pour overhead
}
```

### 4.4 Algorithme

```typescript
export function applySlidingWindow(
  messages: ChatMessage[],
  config: SlidingWindowConfig
): ChatMessage[] {
  // 1. SÃ©parer le system prompt
  // 2. Parcourir du plus rÃ©cent au plus ancien
  // 3. Ajouter tant qu'on reste sous le budget
  // 4. Remettre le system prompt en premier
}
```

### 4.5 RÃ¨gles

| RÃ¨gle                  | Valeur | Raison                              |
| ---------------------- | ------ | ----------------------------------- |
| `preserveSystemPrompt` | true   | Le contexte de base est crucial     |
| `reservedForResponse`  | 1000   | Laisser de la place pour la rÃ©ponse |
| `minMessages`          | 2      | Garder un minimum de contexte       |

### 4.6 IntÃ©gration automatique

```typescript
// Dans MistralService, c'est automatique !
public async complete(messages, options) {
  const { autoTruncate = true } = options;

  if (autoTruncate) {
    messages = applySlidingWindow(messages, {
      maxTokens: getModelContextLimit(model),
      reservedForResponse: 1000,
    });
  }
  // ...
}
```

---

## 5. Streaming avec SSE

### 5.1 Pourquoi le streaming ?

| Sans streaming                          | Avec streaming                         |
| --------------------------------------- | -------------------------------------- |
| Attendre 5-10s pour la rÃ©ponse complÃ¨te | Voir les mots apparaÃ®tre en temps rÃ©el |
| UX frustrante                           | UX fluide comme ChatGPT                |

### 5.2 Server-Sent Events (SSE)

**CÃ´tÃ© serveur :**

```typescript
// conversationController.ts
export async function chat(req: Request, res: Response) {
  // Headers SSE
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');
  res.flushHeaders();

  // Stream les chunks
  for await (const chunk of mistral.streamComplete(messages)) {
    res.write(`data: ${JSON.stringify({ chunk })}\n\n`);
  }

  res.write(`data: ${JSON.stringify({ done: true })}\n\n`);
  res.end();
}
```

### 5.3 AsyncIterable dans MistralService

```typescript
public async *streamComplete(messages): AsyncIterable<string> {
  const stream = await this.client.chat.stream({ model, messages });

  for await (const event of stream) {
    const content = event.data.choices?.[0]?.delta?.content;
    if (content) {
      yield content;  // Retourne chaque morceau
    }
  }
}
```

### 5.4 CÃ´tÃ© frontend (Vue.js)

```typescript
const response = await fetch('/api/chat/stream', { method: 'POST', body });
const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;

  const chunk = decoder.decode(value);
  const lines = chunk.split('\n');

  for (const line of lines) {
    if (line.startsWith('data: ')) {
      const data = JSON.parse(line.substring(6));
      if (data.chunk) {
        messageContent += data.chunk;
        // Mise Ã  jour rÃ©active de l'UI
      }
    }
  }
}
```

---

## 6. Persistance avec Prisma

### 6.1 SchÃ©ma

```prisma
// src/prisma/schema.prisma

model Conversation {
  id        String    @id @default(uuid())
  userId    String?
  title     String?
  createdAt DateTime  @default(now())
  updatedAt DateTime  @updatedAt
  messages  Message[]
}

model Message {
  id             String       @id @default(uuid())
  conversationId String
  role           MessageRole  // user | assistant | system
  content        String
  createdAt      DateTime     @default(now())
  conversation   Conversation @relation(...)
}

enum MessageRole {
  user
  assistant
  system
}
```

### 6.2 ConversationService

```typescript
class ConversationService {
  // CrÃ©er une conversation
  async create(userId?: string): Promise<Conversation>;

  // Ajouter un message (avec transaction)
  async addMessage(conversationId, role, content): Promise<Message>;

  // RÃ©cupÃ©rer l'historique pour Mistral
  async getChatHistory(conversationId): Promise<ChatMessage[]>;
}
```

### 6.3 Transactions Prisma

```typescript
// Garantit l'atomicitÃ© (tout ou rien)
await this.prisma.$transaction([
  this.prisma.message.create({ data: messageData }),
  this.prisma.conversation.update({
    where: { id },
    data: { updatedAt: new Date() },
  }),
]);
```

### 6.4 Niveaux d'isolation

| Niveau          | Description                                         |
| --------------- | --------------------------------------------------- |
| ReadUncommitted | Peut lire des donnÃ©es non validÃ©es                  |
| ReadCommitted   | Ne lit que les donnÃ©es validÃ©es (dÃ©faut PostgreSQL) |
| RepeatableRead  | Lectures cohÃ©rentes dans la transaction             |
| Serializable    | Le plus strict                                      |

---

## 7. Embeddings et Recherche Vectorielle

### 7.1 Qu'est-ce qu'un Embedding ?

Un **embedding** est une reprÃ©sentation vectorielle d'un texte. Il transforme des mots/phrases en tableaux de nombres qui capturent le **sens sÃ©mantique**.

```
"Comment installer Docker ?"
        â†“ generateEmbedding()
[0.023, -0.156, 0.789, 0.034, ...] // 1024 nombres (dimension Mistral)
```

**PropriÃ©tÃ© clÃ©** : Deux textes similaires en sens auront des vecteurs proches dans l'espace.

### 7.2 GÃ©nÃ©ration d'embeddings avec Mistral

```typescript
// src/services/mistral/MistralService.ts

public async generateEmbedding(text: string): Promise<number[]> {
  const response = await this.client.embeddings.create({
    model: 'mistral-embed',
    inputs: [text],
  });
  return response.data[0].embedding; // number[1024]
}

// Version batch (plus efficace pour plusieurs textes)
public async generateEmbeddings(texts: string[]): Promise<number[][]> {
  const response = await this.client.embeddings.create({
    model: 'mistral-embed',
    inputs: texts,
  });
  return response.data.map(item => item.embedding);
}
```

### 7.3 Stockage avec pgvector

**pgvector** est une extension PostgreSQL pour stocker et rechercher des vecteurs.

```sql
-- Activer l'extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Table de documents avec embeddings
CREATE TABLE documents (
  id BIGSERIAL PRIMARY KEY,
  content TEXT,
  embedding vector(1024)  -- Type vector de dimension 1024
);

-- Index pour accÃ©lÃ©rer les recherches (optionnel mais recommandÃ©)
CREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
```

### 7.4 Distance Cosinus

La **distance cosinus** mesure l'angle entre deux vecteurs :

| Distance  | Signification              |
| --------- | -------------------------- |
| 0         | Identiques                 |
| < 0.3     | TrÃ¨s similaires âœ…         |
| 0.3 - 0.6 | LiÃ©s au sujet âš ï¸           |
| > 0.6     | Pas vraiment pertinents âŒ |

```sql
-- OpÃ©rateur <=> pour la distance cosinus dans pgvector
SELECT content, embedding <=> '[0.1, 0.2, ...]'::vector AS distance
FROM documents
ORDER BY distance
LIMIT 5;
```

### 7.5 DocumentService

```typescript
// src/services/document/DocumentService.ts

export class DocumentService {
  // Ajouter un document (embedding gÃ©nÃ©rÃ© automatiquement)
  async addDocument(input: { content: string }): Promise<Document> {
    const embedding = await mistral.generateEmbedding(input.content);
    const embeddingStr = `[${embedding.join(',')}]`;

    // Prisma ne supporte pas nativement pgvector â†’ SQL brut
    return await this.prisma.$queryRawUnsafe(
      `INSERT INTO documents (content, embedding)
       VALUES ($1, $2::vector)
       RETURNING id, content`,
      input.content,
      embeddingStr
    );
  }

  // Recherche sÃ©mantique
  async searchSimilar(query: string, limit = 5): Promise<SearchResult[]> {
    const queryEmbedding = await mistral.generateEmbedding(query);
    const embeddingStr = `[${queryEmbedding.join(',')}]`;

    return await this.prisma.$queryRawUnsafe(
      `SELECT id, content, embedding <=> $1::vector AS distance
       FROM documents
       ORDER BY distance
       LIMIT $2`,
      embeddingStr,
      limit
    );
  }
}
```

### 7.6 Pourquoi $queryRawUnsafe ?

Prisma ne supporte pas nativement le type `vector` de pgvector. On utilise donc :

| MÃ©thode                           | SÃ©curitÃ©     | Usage                                   |
| --------------------------------- | ------------ | --------------------------------------- |
| `prisma.model.create()`           | âœ… SÃ»r       | Types standards (Conversation, Message) |
| `$queryRaw` template              | âœ… SÃ»r       | SQL paramÃ©trÃ© mais limitÃ©               |
| `$queryRawUnsafe` + `$1, $2`      | âœ… SÃ»r       | SQL avec types custom (vector)          |
| `$queryRawUnsafe` + concatÃ©nation | âŒ Dangereux | JAMAIS faire Ã§a !                       |

**Important** : On utilise des paramÃ¨tres positionnels (`$1`, `$2`) qui sont Ã©chappÃ©s par PostgreSQL, donc c'est sÃ©curisÃ©.

### 7.7 Algorithmes de recherche vectorielle

| Algorithme  | Type  | Vitesse        | PrÃ©cision | Usage           |
| ----------- | ----- | -------------- | --------- | --------------- |
| Force brute | Exact | ğŸ¢ Lent        | 100%      | Petits datasets |
| **IVFFlat** | ANN   | ğŸ‡ Rapide      | ~95%      | Bon compromis   |
| **HNSW**    | ANN   | ğŸš€ TrÃ¨s rapide | ~98%      | Production      |

**ANN** = Approximate Nearest Neighbor (sacrifice un peu de prÃ©cision pour la vitesse)

---

## 8. RAG - Retrieval-Augmented Generation

### 8.1 Concept

Le **RAG** permet Ã  un LLM de rÃ©pondre avec des **connaissances externes** (documents privÃ©s, FAQ, etc.) sans fine-tuning.

```
Question utilisateur
        â†“
1. GÃ©nÃ©rer l'embedding de la question
        â†“
2. Chercher les documents similaires en base
        â†“
3. Construire un prompt avec le contexte trouvÃ©
        â†“
4. Envoyer au LLM
        â†“
RÃ©ponse enrichie par les documents
```

### 8.2 Avantages du RAG

| Avantage                         | Description                                |
| -------------------------------- | ------------------------------------------ |
| **DonnÃ©es privÃ©es**              | Le LLM peut rÃ©pondre sur vos docs internes |
| **RÃ©duction des hallucinations** | RÃ©ponses basÃ©es sur des faits              |
| **Pas de fine-tuning**           | Moins coÃ»teux, plus simple                 |
| **Mise Ã  jour facile**           | Ajouter/retirer des docs Ã  chaud           |

### 8.3 ImplÃ©mentation

```typescript
// Exemple de flux RAG complet

async function chatWithRAG(userQuestion: string, conversationId: string) {
  const docs = getDocumentService();
  const mistral = getMistralService();
  const conversations = getConversationService();

  // 1. Chercher les documents pertinents
  const relevantDocs = await docs.searchSimilar(userQuestion, { limit: 3 });

  // 2. Construire le contexte
  const context = relevantDocs.map((doc) => doc.content).join('\n\n---\n\n');

  // 3. CrÃ©er le prompt enrichi
  const systemPrompt = `Tu es un assistant. RÃ©ponds en te basant sur ces documents :

${context}

Si l'information n'est pas dans les documents, dis-le clairement.`;

  // 4. RÃ©cupÃ©rer l'historique et ajouter le system prompt
  const history = await conversations.getChatHistory(conversationId);
  history[0] = { role: 'system', content: systemPrompt };

  // 5. Obtenir la rÃ©ponse
  return await mistral.complete(history);
}
```

### 8.4 Chunking (DÃ©coupage)

Pour les longs documents, on les dÃ©coupe en **chunks** avant de gÃ©nÃ©rer les embeddings.

**Pourquoi ?** Un seul vecteur pour un long document "dilue" le sens. Des chunks permettent une recherche plus prÃ©cise.

### 8.5 StratÃ©gies de chunking

| StratÃ©gie           | Description                | Usage                        |
| ------------------- | -------------------------- | ---------------------------- |
| **Fixed size**      | 500 caractÃ¨res             | Simple, rapide               |
| **Sentence-based**  | Par phrases                | PrÃ©serve le sens             |
| **Paragraph-based** | Par paragraphes            | Documents structurÃ©s         |
| **Overlap**         | Chevauchement entre chunks | Ã‰vite de couper des idÃ©es âœ… |
| **Semantic**        | Par similaritÃ© sÃ©mantique  | Le plus prÃ©cis, mais coÃ»teux |

### 8.6 Chunking avec Overlap (ImplÃ©mentÃ©)

Le **chevauchement (overlap)** est crucial : il permet de prÃ©server le contexte entre les chunks.

```
Document : |-------- 1200 caractÃ¨res --------|

Sans overlap :
  Chunk 1: |--- 500 ---|
  Chunk 2:             |--- 500 ---|   â† Coupure nette, perte de contexte !
  Chunk 3:                         |--- 200 ---|

Avec overlap (100 chars) :
  Chunk 1: |--- 500 ---|
  Chunk 2:        |--- 500 ---|   â† Les 100 derniers chars de Chunk 1
                                     sont les 100 premiers de Chunk 2
  Chunk 3:              |--- 500 ---|
```

**Notre implÃ©mentation** :

```typescript
// src/application/services/chunking/ChunkingService.ts

export class ChunkingService {
  /**
   * DÃ©coupe un texte en chunks avec chevauchement
   *
   * @param text - Le texte Ã  dÃ©couper
   * @param options.chunkSize - Taille max d'un chunk (dÃ©faut: 500)
   * @param options.overlap - Chevauchement entre chunks (dÃ©faut: 100)
   * @param options.separators - SÃ©parateurs pour couper proprement
   */
  chunkText(text: string, options: ChunkingOptions = {}): ChunkingResult {
    const {
      chunkSize = 500,
      overlap = 100,
      separators = ['\n\n', '\n', '. ', ' '],
    } = options;

    // Validation : l'overlap doit Ãªtre < chunkSize
    if (overlap >= chunkSize) {
      throw new Error('Overlap must be smaller than chunkSize');
    }

    const chunks: Chunk[] = [];
    let currentPosition = 0;

    while (currentPosition < text.length) {
      // 1. DÃ©finir la fin potentielle du chunk
      let endPosition = Math.min(currentPosition + chunkSize, text.length);

      // 2. Chercher un bon point de coupure (sÃ©parateur)
      if (endPosition < text.length) {
        const chunkContent = text.slice(currentPosition, endPosition);
        const splitPoint = this.findBestSplitPoint(chunkContent, separators);
        if (splitPoint > chunkSize / 2) {
          endPosition = currentPosition + splitPoint;
        }
      }

      // 3. Extraire et sauvegarder le chunk
      chunks.push({
        content: text.slice(currentPosition, endPosition).trim(),
        index: chunks.length,
        startOffset: currentPosition,
        endOffset: endPosition,
      });

      // 4. Avancer avec overlap : step = chunkSize - overlap
      currentPosition += chunkSize - overlap;
    }

    return { chunks, totalChunks: chunks.length, originalLength: text.length };
  }

  private findBestSplitPoint(text: string, separators: string[]): number {
    // Cherche le dernier sÃ©parateur de haute prioritÃ©
    for (const separator of separators) {
      const lastIndex = text.lastIndexOf(separator);
      if (lastIndex !== -1) return lastIndex + separator.length;
    }
    return -1;
  }
}
```

**Endpoint API** :

```bash
# POST /api/documents/chunked
curl -X POST http://localhost:3000/api/documents/chunked \
  -H "Content-Type: application/json" \
  -d '{
    "content": "TrÃ¨s long document de 5000 mots...",
    "chunkSize": 500,
    "overlap": 100
  }'

# RÃ©ponse :
{
  "message": "Document split into 12 chunks",
  "sourceId": 27,                          # ID du document source (original)
  "totalChunks": 12,
  "originalLength": 5000,
  "documents": [
    { "id": 28, "sourceId": 27, "chunkIndex": 0, "contentPreview": "..." },
    { "id": 29, "sourceId": 27, "chunkIndex": 1, "contentPreview": "..." },
    ...
  ],
  "chunks": [...]
}
```

**Tracking des chunks en base de donnÃ©es** :

```sql
-- Structure de la table documents avec tracking
SELECT id, LEFT(content, 30), source_id, chunk_index, embedding IS NOT NULL
FROM documents WHERE id >= 27;

 id |           left           | source_id | chunk_index | has_embedding
----+--------------------------+-----------+-------------+---------------
 27 | Le machine learning est  |     NULL  |        NULL | false  -- Document source
 28 | Le machine learning est  |        27 |           0 | true   -- Chunk 0
 29 | Les algorithmes de ML... |        27 |           1 | true   -- Chunk 1
 30 | Le deep learning est...  |        27 |           2 | true   -- Chunk 2
```

**Suppression en cascade** : Supprimer le document source (id=27) supprime automatiquement tous ses chunks grÃ¢ce Ã  `ON DELETE CASCADE`.

```bash
curl -X DELETE http://localhost:3000/api/documents/27
# â†’ Les documents 28, 29, 30 sont automatiquement supprimÃ©s
```

**Calcul du step** :

```
step = chunkSize - overlap = 500 - 100 = 400

Document de 1200 caractÃ¨res :
  Chunk 1 : position 0 â†’ 500 (500 chars)
  Chunk 2 : position 400 â†’ 900 (500 chars)
  Chunk 3 : position 800 â†’ 1200 (400 chars)

Estimation : ceil((1200 - 100) / 400) = ceil(2.75) = 3 chunks
```

**Avantages de l'overlap** :

| Avantage                     | Explication                                                |
| ---------------------------- | ---------------------------------------------------------- |
| **PrÃ©servation du contexte** | Une phrase coupÃ©e en deux sera complÃ¨te dans un des chunks |
| **Meilleure recherche**      | Plus de chances de retrouver l'info pertinente             |
| **CohÃ©rence sÃ©mantique**     | Les embeddings captent mieux le sens                       |
| **CoÃ»t minimal**             | ~20% de tokens en plus, mais qualitÃ© bien meilleure        |

### 8.7 ModÃ¨les d'embedding

| ModÃ¨le                   | Fournisseur | Dimension | CoÃ»t             |
| ------------------------ | ----------- | --------- | ---------------- |
| `mistral-embed`          | Mistral     | 1024      | ~0.1â‚¬/1M tokens  |
| `text-embedding-3-small` | OpenAI      | 1536      | ~0.02$/1M tokens |
| `text-embedding-3-large` | OpenAI      | 3072      | ~0.13$/1M tokens |
| `all-MiniLM-L6-v2`       | Open source | 384       | Gratuit (local)  |
| `nomic-embed-text`       | Open source | 768       | Gratuit (local)  |

### 8.8 Le System Prompt est rÃ©Ã©crit Ã  chaque message

**Point clÃ©** : Le system prompt n'est pas statique. Il est **enrichi dynamiquement** Ã  chaque nouveau message avec les documents pertinents pour cette question spÃ©cifique.

```typescript
// Ã€ CHAQUE message de l'utilisateur :

// 1. RÃ©cupÃ©rer l'historique (avec le system prompt original)
const chatHistory = await conversationService.getChatHistory(conversationId);
// chatHistory[0] = { role: 'system', content: 'Tu es un assistant...' }

// 2. Construire un NOUVEAU system prompt basÃ© sur la question
const ragContext = await ragService.buildEnrichedPrompt(message);
// ragContext.enrichedPrompt = 'Tu es un assistant... [Document 1] WiFi = Secret123...'

// 3. REMPLACER le system prompt original
chatHistory[0].content = ragContext.enrichedPrompt;

// 4. Envoyer Ã  Mistral avec le nouveau contexte
mistralClient.streamComplete(chatHistory);
```

**Pourquoi ?** Chaque question peut nÃ©cessiter des documents diffÃ©rents :

```
Message 1 : "Salut !"
  â†’ RAG cherche docs pour "Salut" â†’ Rien de pertinent
  â†’ System prompt = basique

Message 2 : "C'est quoi le WiFi ?"
  â†’ RAG cherche docs pour "WiFi" â†’ Trouve le doc WiFi !
  â†’ System prompt = enrichi avec infos WiFi

Message 3 : "Et les horaires ?"
  â†’ RAG cherche docs pour "horaires" â†’ Trouve le doc horaires !
  â†’ System prompt = enrichi avec infos horaires (diffÃ©rent !)
```

**Note** : L'enrichissement est fait **Ã  la volÃ©e**. Le system prompt original en base de donnÃ©es n'est jamais modifiÃ©.

### 8.9 Exemple complet de requÃªte Mistral avec RAG

Voici exactement ce qui est envoyÃ© Ã  l'API Mistral quand le RAG trouve un document :

**ScÃ©nario** : L'utilisateur demande "C'est quoi le mot de passe WiFi ?" et un document existe en base avec cette info.

```json
{
  "model": "mistral-tiny",
  "temperature": 0.7,
  "stream": true,
  "messages": [
    {
      "role": "system",
      "content": "Tu es un assistant IA amical et serviable. Tu rÃ©ponds en franÃ§ais de maniÃ¨re concise et utile.\n\nTu as accÃ¨s aux documents suivants pour t'aider Ã  rÃ©pondre :\n\n[Document 1]\nLe mot de passe WiFi du bureau est SuperSecret123. Le rÃ©seau s'appelle BureauNet.\n\nInstructions :\n- Utilise ces documents pour rÃ©pondre si pertinent\n- Si l'information n'est pas dans les documents, utilise tes connaissances gÃ©nÃ©rales\n- Ne mentionne pas explicitement \"selon les documents\" sauf si l'utilisateur le demande"
    },
    {
      "role": "user",
      "content": "Salut !"
    },
    {
      "role": "assistant",
      "content": "Bonjour ! Comment puis-je t'aider ?"
    },
    {
      "role": "user",
      "content": "C'est quoi le mot de passe WiFi ?"
    }
  ]
}
```

**Points importants** :

- Le contenu du document est **littÃ©ralement copiÃ©** dans le system prompt
- L'**historique complet** de conversation est envoyÃ©
- Le RAG est basÃ© sur la **derniÃ¨re question** uniquement
- Mistral "voit" les documents comme du texte normal dans le system prompt

**RÃ©ponse de Mistral** :

```json
{
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "Le mot de passe WiFi est SuperSecret123 et le rÃ©seau s'appelle BureauNet."
      }
    }
  ]
}
```

### 8.9 CoÃ»t du RAG

Le RAG consomme des tokens supplÃ©mentaires car les documents sont envoyÃ©s Ã  chaque requÃªte :

| Composant                   | Tokens (exemple) |
| --------------------------- | ---------------- |
| System prompt de base       | ~50              |
| Documents injectÃ©s (3 docs) | ~300-500         |
| Historique conversation     | ~200             |
| Question utilisateur        | ~20              |
| **Total entrÃ©e**            | **~600-800**     |
| RÃ©ponse                     | ~100             |

**CoÃ»t approximatif** : ~0.001â‚¬ par message avec RAG (mistral-tiny)

### 8.10 Reranking - AmÃ©liorer la prÃ©cision des rÃ©sultats

#### 8.10.1 Le problÃ¨me avec la recherche vectorielle seule

La recherche vectorielle (embedding + distance cosinus) est **rapide et scalable**, mais elle a des limites :

| Aspect        | Recherche vectorielle   | Limitation                |
| ------------- | ----------------------- | ------------------------- |
| **MÃ©thode**   | Compare les embeddings  | Approximation du sens     |
| **Vitesse**   | ~10ms pour 10K docs     | âœ… Rapide                 |
| **PrÃ©cision** | ~80-90%                 | âš ï¸ Peut rater des nuances |
| **Contexte**  | Encode texte sÃ©parÃ©ment | âŒ Pas de cross-attention |

**Exemple du problÃ¨me** :

```
Question : "mot de passe WiFi"
Document 1 : "Le WiFi du bureau est BureauNet" â†’ Distance 0.3 âœ…
Document 2 : "Le mot de passe WiFi est Secret123" â†’ Distance 0.35 âš ï¸

La recherche vectorielle peut mal classer ces documents car les embeddings
sont gÃ©nÃ©rÃ©s SÃ‰PARÃ‰MENT pour la question et chaque document.
```

#### 8.10.2 La solution : Cross-Encoder Reranking

Un **Cross-Encoder** analyse la question ET le document **ensemble**, ce qui permet de capturer les relations fines entre eux.

```
Recherche vectorielle (rapide, large net)
            â†“
      10 candidats
            â†“
Cross-Encoder (prÃ©cis, analyse paire par paire)
            â†“
      Top 3 rerankÃ©s
            â†“
    Contexte pour le LLM
```

**DiffÃ©rence fondamentale** :

| Bi-Encoder (Embeddings)     | Cross-Encoder (Reranking) |
| --------------------------- | ------------------------- |
| Encode Q et D sÃ©parÃ©ment    | Encode (Q, D) ensemble    |
| `embed(Q)` â†” `embed(D)`    | `score(Q, D)`             |
| Rapide (~1ms par doc)       | Lent (~50ms par paire)    |
| Scalable (millions de docs) | Top-K seulement           |
| PrÃ©cision ~85%              | PrÃ©cision ~95%+           |

#### 8.10.3 Choix technique : bge-reranker-base

Nous avons choisi le modÃ¨le **BAAI/bge-reranker-base** :

| CritÃ¨re         | bge-reranker-base   | Alternatives      |
| --------------- | ------------------- | ----------------- |
| **Taille**      | 278 MB              | large: 560 MB     |
| **PrÃ©cision**   | â­â­â­â­ TrÃ¨s bonne | large: â­â­â­â­â­ |
| **Vitesse**     | ~50ms / 10 docs     | large: ~100ms     |
| **Licence**     | MIT (libre)         | âœ…                |
| **Multilingue** | FranÃ§ais âœ…         | âœ…                |

**Pourquoi pas un modÃ¨le plus gros ?**

- Le gain de prÃ©cision entre `base` et `large` est marginal (~2-3%)
- La latence double
- Pour notre cas d'usage (FAQ interne), `base` est suffisant

**Alternatives considÃ©rÃ©es** :

| Option                | Avantage                      | InconvÃ©nient               | Choix     |
| --------------------- | ----------------------------- | -------------------------- | --------- |
| **bge-reranker**      | Open source, local            | NÃ©cessite Python           | âœ… Choisi |
| **Cohere Rerank API** | TrÃ¨s prÃ©cis                   | Payant, dÃ©pendance externe | âŒ        |
| **LLM Reranking**     | Utilise Mistral dÃ©jÃ  en place | Lent, coÃ»teux en tokens    | âŒ        |
| **ms-marco-MiniLM**   | LÃ©ger (66MB)                  | Moins prÃ©cis, anglais      | âŒ        |

#### 8.10.4 Architecture du service de reranking

Nous avons crÃ©Ã© un **microservice Python sÃ©parÃ©** plutÃ´t que d'intÃ©grer le modÃ¨le dans Node.js :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Node.js (App principale)                  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ RAGService                                                â”‚   â”‚
â”‚  â”‚   1. searchByQuery() â†’ 10 candidats (pgvector)           â”‚   â”‚
â”‚  â”‚   2. rerankClient.rerank() â†’ appel HTTP                  â”‚   â”‚
â”‚  â”‚   3. Top 3 rerankÃ©s â†’ contexte enrichi                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ HTTP POST /rerank
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Python (Rerank Service)                       â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ FastAPI + CrossEncoder                                    â”‚   â”‚
â”‚  â”‚   - ModÃ¨le chargÃ© en mÃ©moire au dÃ©marrage                â”‚   â”‚
â”‚  â”‚   - POST /rerank : score chaque paire (query, doc)       â”‚   â”‚
â”‚  â”‚   - Retourne les docs triÃ©s par score                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  ModÃ¨le : BAAI/bge-reranker-base (~278 MB en RAM)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pourquoi un service Python sÃ©parÃ© ?**

| Raison                  | Explication                                     |
| ----------------------- | ----------------------------------------------- |
| **Ã‰cosystÃ¨me ML**       | PyTorch, Transformers sont natifs Python        |
| **Isolation**           | Le modÃ¨le ne bloque pas l'event loop Node.js    |
| **Scaling indÃ©pendant** | On peut scaler le reranker sÃ©parÃ©ment           |
| **Fallback gracieux**   | Si le service tombe, on continue sans reranking |

#### 8.10.5 ImplÃ©mentation dÃ©taillÃ©e

**Service Python (FastAPI)** :

```python
# rerank-service/main.py

from sentence_transformers import CrossEncoder
from fastapi import FastAPI
from pydantic import BaseModel

# Chargement du modÃ¨le au dÃ©marrage (une seule fois)
model = CrossEncoder('BAAI/bge-reranker-base', max_length=512)

class RerankRequest(BaseModel):
    query: str
    documents: list[dict]  # [{id, content}, ...]
    top_k: int = 3

@app.post("/rerank")
async def rerank(request: RerankRequest):
    # 1. CrÃ©er les paires (query, document)
    pairs = [(request.query, doc["content"]) for doc in request.documents]

    # 2. Scorer chaque paire avec le cross-encoder
    scores = model.predict(pairs)  # Retourne un score pour chaque paire

    # 3. Trier par score dÃ©croissant et prendre le top_k
    results = sorted(zip(request.documents, scores),
                     key=lambda x: x[1], reverse=True)

    return {"results": results[:request.top_k]}
```

**Client TypeScript** :

```typescript
// src/infrastructure/external/rerank/RerankClient.ts

export class RerankClient implements IRerankClient {
  private readonly serviceUrl: string;
  private available: boolean | null = null; // Cache de disponibilitÃ©

  constructor() {
    this.serviceUrl = process.env.RERANK_SERVICE_URL!;
  }

  async isAvailable(): Promise<boolean> {
    // Cache le rÃ©sultat pour Ã©viter des appels rÃ©pÃ©tÃ©s
    if (this.available !== null) return this.available;

    try {
      const res = await fetch(`${this.serviceUrl}/health`);
      this.available = res.ok;
    } catch {
      this.available = false;
    }
    return this.available;
  }

  async rerank(
    query: string,
    documents: RerankDocument[],
    topK: number
  ): Promise<RerankResult[]> {
    const response = await fetch(`${this.serviceUrl}/rerank`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ query, documents, top_k: topK }),
    });

    const data = await response.json();
    return data.results;
  }
}
```

**IntÃ©gration dans RAGService** :

```typescript
// src/application/services/rag/RAGService.ts

async buildEnrichedPrompt(userMessage: string): Promise<RAGContext> {
  // 1. Recherche vectorielle large (10 candidats)
  const candidates = await documentService.searchByQuery(userMessage, {
    limit: this.config.rerankCandidates,  // 10
    maxDistance: this.config.maxDistance,
  });

  // 2. Reranking si disponible
  let finalChunks: ChunkWithDistance[];

  if (this.config.useReranking && isRerankConfigured()) {
    const reranked = await this.rerankChunks(userMessage, candidates);
    finalChunks = reranked.chunks;  // Top 3 aprÃ¨s reranking
  } else {
    // Fallback : prendre les premiers rÃ©sultats vectoriels
    finalChunks = candidates.slice(0, this.config.maxDocuments);
  }

  // 3. Construire le contexte avec les documents rerankÃ©s
  return this.buildContext(finalChunks);
}

private async rerankChunks(query: string, chunks: ChunkWithDistance[]) {
  const rerankClient = getRerankClient();

  // VÃ©rifier la disponibilitÃ© (avec cache)
  if (!await rerankClient.isAvailable()) {
    console.warn('âš ï¸ Rerank service not available, using vector search only');
    return this.fallbackToVectorSearch(chunks);
  }

  // PrÃ©parer les documents pour le reranking
  const documents = chunks.map(c => ({ id: c.id, content: c.content }));

  // Appeler le service
  const results = await rerankClient.rerank(query, documents, this.config.maxDocuments);

  // Mapper les rÃ©sultats aux chunks originaux
  return this.mapRerankResults(results, chunks);
}
```

#### 8.10.6 Configuration

```typescript
// src/application/services/rag/types.ts

export interface RAGConfig {
  maxDocuments: number; // Nombre final de docs (dÃ©faut: 3)
  maxDistance: number; // Distance max pour recherche vectorielle (dÃ©faut: 0.8)
  useReranking: boolean; // Activer le reranking (dÃ©faut: true)
  rerankCandidates: number; // Candidats avant reranking (dÃ©faut: 10)
}

export const DEFAULT_RAG_CONFIG: RAGConfig = {
  maxDocuments: 3,
  maxDistance: 0.8,
  useReranking: true,
  rerankCandidates: 10,
};
```

**Variables d'environnement** :

```bash
# docker-compose.yml
RERANK_SERVICE_URL=http://rerank:8001
```

#### 8.10.7 Fallback gracieux

Le systÃ¨me est conÃ§u pour **continuer Ã  fonctionner** mÃªme si le service de reranking est indisponible :

```
                    Service Rerank
                         â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                         â”‚
        Disponible âœ…            Indisponible âŒ
            â”‚                         â”‚
     Cross-encoder            Fallback vectoriel
     reranking                (top 3 par distance)
            â”‚                         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                  Contexte enrichi
                         â”‚
                      Mistral
```

**Comportement en cas d'erreur** :

| Situation           | Comportement       | Log                               |
| ------------------- | ------------------ | --------------------------------- |
| Service non dÃ©marrÃ© | Fallback vectoriel | `âš ï¸ Rerank service not available` |
| Timeout             | Fallback vectoriel | `Rerank service timeout`          |
| Erreur 500          | Fallback vectoriel | `Reranking failed, falling back`  |
| Service OK          | Reranking normal   | `ğŸ”„ Reranked 10 â†’ 3 chunks`       |

#### 8.10.8 Scores et similaritÃ©

Le cross-encoder retourne des **scores bruts** (logits) qu'on convertit en pourcentage :

```typescript
// Score cross-encoder : entre -10 et +10 typiquement
// On applique une sigmoid pour normaliser en 0-100%

function rerankScoreToSimilarity(score: number): number {
  const normalized = 1 / (1 + Math.exp(-score)); // Sigmoid
  return Math.round(normalized * 100);
}

// Exemples :
// score = 5.0  â†’ 99%  (trÃ¨s pertinent)
// score = 0.0  â†’ 50%  (neutre)
// score = -5.0 â†’ 1%   (pas pertinent)
```

**DiffÃ©rence avec la distance cosinus** :

| MÃ©trique         | InterprÃ©tation             | Origine          |
| ---------------- | -------------------------- | ---------------- |
| Distance cosinus | 0 = identique, 2 = opposÃ©  | Embeddings       |
| Score reranking  | Plus haut = plus pertinent | Cross-encoder    |
| SimilaritÃ© %     | Plus haut = plus pertinent | Notre conversion |

#### 8.10.9 Performance et latence

| Ã‰tape                           | Latence typique | Impact       |
| ------------------------------- | --------------- | ------------ |
| Recherche vectorielle (10 docs) | ~10-50ms        | Minimal      |
| Appel HTTP au service rerank    | ~5ms            | RÃ©seau local |
| Cross-encoder (10 paires)       | ~50-100ms       | Principal    |
| **Total avec reranking**        | **~70-150ms**   | Acceptable   |
| Sans reranking                  | ~15-50ms        | Plus rapide  |

**Optimisations possibles** :

1. **Batching** : Grouper plusieurs requÃªtes de reranking
2. **GPU** : Utiliser CUDA pour accÃ©lÃ©rer le modÃ¨le
3. **ModÃ¨le plus lÃ©ger** : `bge-reranker-small` si latence critique
4. **Cache** : Mettre en cache les rÃ©sultats de reranking

#### 8.10.11 Docker et dÃ©ploiement

```yaml
# docker-compose.yml

services:
  rerank:
    image: ia-project-rerank:latest
    build:
      context: ./rerank-service
      dockerfile: Dockerfile
    ports:
      - '8001:8001'
    environment:
      - RERANK_MODEL=BAAI/bge-reranker-base
      - DEFAULT_TOP_K=3
    volumes:
      - rerank_cache:/root/.cache # Cache du modÃ¨le HuggingFace
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8001/health']
      interval: 30s
      start_period: 60s # Temps pour charger le modÃ¨le
```

**Commandes utiles** :

```bash
# Construire l'image (tÃ©lÃ©charge le modÃ¨le)
docker build --network=host -t ia-project-rerank ./rerank-service

# DÃ©marrer le service
docker compose up -d rerank

# VÃ©rifier la santÃ©
curl http://localhost:8001/health
# {"status":"healthy","model":"BAAI/bge-reranker-base","model_loaded":true}

# Tester le reranking
curl -X POST http://localhost:8001/rerank \
  -H "Content-Type: application/json" \
  -d '{
    "query": "mot de passe wifi",
    "documents": [
      {"id": 1, "content": "Les horaires sont 8h-20h"},
      {"id": 2, "content": "Le mot de passe WiFi est Secret123"}
    ],
    "top_k": 2
  }'
# {"results":[{"id":2,"score":0.99,...},{"id":1,"score":0.01,...}]}
```

#### 8.10.12 Points clÃ©s Ã  retenir

| Concept                         | Explication                                  |
| ------------------------------- | -------------------------------------------- |
| **Bi-Encoder vs Cross-Encoder** | Embeddings sÃ©parÃ©s vs analyse conjointe      |
| **Two-stage retrieval**         | Recherche large puis reranking prÃ©cis        |
| **Fallback gracieux**           | Le systÃ¨me fonctionne sans reranking         |
| **Microservice**                | Isolation du modÃ¨le ML dans un service dÃ©diÃ© |
| **Score â†’ SimilaritÃ©**          | Sigmoid pour normaliser les logits           |
| **Cache de disponibilitÃ©**      | Ã‰vite les appels rÃ©pÃ©tÃ©s Ã  /health           |

---

## 9. Query Rewriting - Reformulation de requÃªtes

### 9.1 Le problÃ¨me

Les utilisateurs formulent souvent leurs questions de maniÃ¨re **imprÃ©cise** :

| ProblÃ¨me             | Exemple                | Impact sur la recherche    |
| -------------------- | ---------------------- | -------------------------- |
| **AbrÃ©viations**     | "mdp wifi ?"           | Embedding â‰  "mot de passe" |
| **Questions vagues** | "Ã§a marche comment ?"  | Pas de contexte            |
| **Pronoms**          | "c'est quoi le prix ?" | RÃ©fÃ©rence ambiguÃ«          |
| **Fautes/typos**     | "commnet installer"    | Embedding dÃ©gradÃ©          |

### 9.2 La solution : Query Rewriting

Le **Query Rewriting** utilise le LLM pour reformuler la requÃªte **avant** la recherche vectorielle :

```
User: "mdp wifi ?"
        â”‚
        â–¼ [Query Rewriting - Mistral]
"Quel est le mot de passe du rÃ©seau WiFi ?"
        â”‚
        â–¼ [Embedding + Recherche]
Documents pertinents trouvÃ©s âœ…
```

### 9.3 ImplÃ©mentation

**Interface (Port)** :

```typescript
// src/application/ports/out/IQueryRewriterService.ts

export interface QueryRewriteResult {
  originalQuery: string;
  rewrittenQuery: string;
  wasRewritten: boolean;
}

export interface IQueryRewriterService {
  rewrite(
    query: string,
    conversationContext?: string[]
  ): Promise<QueryRewriteResult>;
}
```

**Service** :

```typescript
// src/application/services/queryRewriter/QueryRewriterService.ts

const REWRITE_SYSTEM_PROMPT = `Tu es un expert en reformulation de requÃªtes...

RÃˆGLES:
- DÃ©veloppe les abrÃ©viations (mdp â†’ mot de passe, wifi â†’ rÃ©seau WiFi...)
- Reformule les questions vagues en questions prÃ©cises
- Si la question fait rÃ©fÃ©rence au contexte (Ã§a, il, elle), inclus explicitement le sujet
- Garde la requÃªte concise (max 30 mots)
- RÃ©ponds UNIQUEMENT avec la question reformulÃ©e`;

export class QueryRewriterService implements IQueryRewriterService {
  async rewrite(
    query: string,
    conversationContext: string[] = []
  ): Promise<QueryRewriteResult> {
    // Skip si query trop courte
    if (query.trim().length < 2) {
      return {
        originalQuery: query,
        rewrittenQuery: query,
        wasRewritten: false,
      };
    }

    try {
      const userPrompt = this.buildUserPrompt(query, conversationContext);

      const rewrittenQuery = await this.mistralClient.complete(
        [
          { role: 'system', content: REWRITE_SYSTEM_PROMPT },
          { role: 'user', content: userPrompt },
        ],
        { temperature: 0.1, maxTokens: 100 }
      );

      console.log(`âœï¸ Query rewrite: "${query}" â†’ "${rewrittenQuery}"`);

      return {
        originalQuery: query,
        rewrittenQuery: rewrittenQuery.trim(),
        wasRewritten: true,
      };
    } catch (error) {
      // Fallback : utiliser la query originale
      return {
        originalQuery: query,
        rewrittenQuery: query,
        wasRewritten: false,
      };
    }
  }

  private buildUserPrompt(query: string, context: string[]): string {
    if (context.length > 0) {
      return `CONTEXTE CONVERSATIONNEL:
${context
  .slice(-3)
  .map((msg, i) => `${i + 1}. ${msg}`)
  .join('\n')}

QUESTION Ã€ REFORMULER:
${query}`;
    }
    return `QUESTION Ã€ REFORMULER:\n${query}`;
  }
}
```

### 9.4 IntÃ©gration dans le RAG

```typescript
// src/application/services/rag/RAGService.ts

async buildEnrichedPrompt(
  userMessage: string,
  options: RAGOptions = {}
): Promise<RAGContext> {
  // Ã‰tape 0: Query Rewriting (si activÃ©)
  const shouldRewrite = options.useQueryRewrite ?? true;
  let searchQuery = userMessage;

  if (shouldRewrite) {
    const queryRewriter = getQueryRewriterService();
    const rewriteResult = await queryRewriter.rewrite(
      userMessage,
      options.conversationHistory ?? []
    );
    searchQuery = rewriteResult.rewrittenQuery;
  }

  // Ã‰tape 1: Recherche avec la query rÃ©Ã©crite
  const candidates = await documentService.searchByQuery(searchQuery, {
    limit: searchLimit,
    maxDistance: this.config.maxDistance,
  });

  // ... reste du pipeline (reranking, enrichissement)
}
```

### 9.5 Utilisation du contexte conversationnel

Le Query Rewriter reÃ§oit les **derniers messages utilisateur** pour rÃ©soudre les rÃ©fÃ©rences :

```
Conversation :
  User: "Comment se connecter au WiFi ?"
  Assistant: "Allez dans ParamÃ¨tres > WiFi > BureauNet"
  User: "c'est quoi le mdp ?"  â† RÃ©fÃ©rence implicite au WiFi

Query Rewriting avec contexte :
  "c'est quoi le mdp ?" â†’ "Quel est le mot de passe du rÃ©seau WiFi BureauNet ?"
```

### 9.6 Configuration

```typescript
// src/application/services/queryRewriter/types.ts

export interface QueryRewriterConfig {
  enabled: boolean; // Activer/dÃ©sactiver
  minQueryLength: number; // Longueur min pour rewriting (dÃ©faut: 2)
  maxContextMessages: number; // Nb messages de contexte (dÃ©faut: 3)
}

export const DEFAULT_QUERY_REWRITER_CONFIG: QueryRewriterConfig = {
  enabled: true,
  minQueryLength: 2,
  maxContextMessages: 3,
};
```

### 9.7 Toggle Frontend

L'interface de chat propose un toggle âœï¸ **Rewrite** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â˜‘ ğŸ“š RAG    â˜‘ âœï¸ Rewrite    â˜‘ ğŸ”„ Rerank   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Ã‰cris ton message...                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.8 Performance et coÃ»t

| Aspect        | Impact                                |
| ------------- | ------------------------------------- |
| **Latence**   | +200-400ms (appel LLM)                |
| **CoÃ»t**      | ~0.0001â‚¬ par requÃªte (mistral-small)  |
| **PrÃ©cision** | +15-30% de recall sur queries courtes |

**Quand dÃ©sactiver ?**

- Queries dÃ©jÃ  bien formulÃ©es
- Latence critique
- CoÃ»t Ã  minimiser

### 9.9 Exemples de reformulation

| Original       | ReformulÃ©                                         |
| -------------- | ------------------------------------------------- |
| "mdp"          | "Quel est le mot de passe ?"                      |
| "wifi ?"       | "Comment se connecter au rÃ©seau WiFi ?"           |
| "horaires"     | "Quels sont les horaires d'ouverture ?"           |
| "c koi docker" | "Qu'est-ce que Docker et comment Ã§a fonctionne ?" |
| "et le prix ?" | "Quel est le prix de [sujet prÃ©cÃ©dent] ?"         |

---

## 10. Hybrid Search - Recherche hybride

### 10.1 Le problÃ¨me avec la recherche vectorielle seule

La recherche vectorielle (embeddings + distance cosinus) est excellente pour trouver des documents **sÃ©mantiquement similaires**, mais elle a des limites :

| Type de requÃªte    | Exemple                       | Recherche vectorielle |
| ------------------ | ----------------------------- | --------------------- |
| **Concepts**       | "Comment fonctionne Docker ?" | âœ… Excellent          |
| **Synonymes**      | "conteneurs" vs "containers"  | âœ… Bon                |
| **Codes produits** | "XR-7500"                     | âŒ Peut rater         |
| **Noms propres**   | "Jean Dupont"                 | âŒ Peut rater         |
| **Acronymes**      | "FAQ", "API"                  | âš ï¸ Variable           |
| **Mots exacts**    | "erreur 404"                  | âš ï¸ Variable           |

**Pourquoi ?** Les embeddings capturent le **sens**, pas les **mots exacts**. Un code produit "XR-7500" n'a pas de sens sÃ©mantique intrinsÃ¨que.

### 10.2 La solution : combiner Vector + Keyword

Le **Hybrid Search** combine deux mÃ©thodes de recherche :

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Query          â”‚
                    â”‚  "mot de passe XR"  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Vector Search   â”‚             â”‚ Keyword Search  â”‚
    â”‚ (Embeddings)    â”‚             â”‚ (Full-Text)     â”‚
    â”‚                 â”‚             â”‚                 â”‚
    â”‚ "sens global"   â”‚             â”‚ "mots exacts"   â”‚
    â”‚ Trouve: pwd,    â”‚             â”‚ Trouve: XR-7500 â”‚
    â”‚ password...     â”‚             â”‚ exactement      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                               â”‚
             â”‚   RÃ©sultats par               â”‚  RÃ©sultats par
             â”‚   distance cosinus            â”‚  score BM25
             â”‚                               â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Fusion (RRF)   â”‚
                   â”‚  Combine les    â”‚
                   â”‚  deux rankings  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ RÃ©sultats finauxâ”‚
                   â”‚ (best of both)  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.3 Full-Text Search dans PostgreSQL

PostgreSQL offre un support natif puissant pour la recherche textuelle via les types `tsvector` et `tsquery`.

#### 10.3.1 tsvector - Le vecteur de recherche

Un `tsvector` est une reprÃ©sentation optimisÃ©e d'un texte pour la recherche :

```sql
SELECT to_tsvector('french', 'Le mot de passe WiFi est SuperSecret123');
-- RÃ©sultat : 'mot':2 'pass':4 'supersecret123':6 'wifi':5
```

**Ce qui se passe** :

1. **Tokenisation** : Le texte est dÃ©coupÃ© en mots
2. **Normalisation** : Conversion en minuscules
3. **Stemming** : RÃ©duction aux racines (ex: "passes" â†’ "pass")
4. **Stop words** : Suppression des mots vides ("le", "est", "de"...)
5. **Positions** : Chaque token garde sa position dans le texte

#### 10.3.2 tsquery - La requÃªte de recherche

Une `tsquery` est une requÃªte de recherche textuelle :

```sql
SELECT plainto_tsquery('french', 'mot de passe wifi');
-- RÃ©sultat : 'mot' & 'pass' & 'wifi'

-- OpÃ©rateurs disponibles :
-- & : AND (tous les termes)
-- | : OR (au moins un terme)
-- ! : NOT (exclure un terme)
-- <-> : FOLLOWED BY (ordre)
```

#### 10.3.3 L'opÃ©rateur @@ (matching)

L'opÃ©rateur `@@` teste si un `tsvector` correspond Ã  une `tsquery` :

```sql
SELECT 'mot':1 'pass':2 'wifi':3'::tsvector @@ 'wifi & pass'::tsquery;
-- RÃ©sultat : true

SELECT 'mot':1 'pass':2 'wifi':3'::tsvector @@ 'docker'::tsquery;
-- RÃ©sultat : false
```

#### 10.3.4 ts_rank - Le scoring

`ts_rank` calcule un score de pertinence :

```sql
SELECT ts_rank(
  to_tsvector('french', 'Le mot de passe WiFi est SuperSecret'),
  plainto_tsquery('french', 'mot de passe')
) as rank;
-- RÃ©sultat : 0.0991 (plus c'est haut, plus c'est pertinent)
```

**Facteurs de ranking** :

- FrÃ©quence des termes
- ProximitÃ© des termes
- Position dans le document
- Longueur du document

#### 10.3.5 Index GIN

L'index **GIN** (Generalized Inverted Index) accÃ©lÃ¨re drastiquement les recherches :

```sql
-- Sans index : scan sÃ©quentiel O(n)
-- Avec index GIN : recherche O(log n)

CREATE INDEX chunks_search_idx ON chunks USING GIN(search_vector);
```

**Comment Ã§a marche ?**

```
GIN Index (inversÃ©)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"wifi"    â†’ [doc_1, doc_5, doc_12]
"passe"   â†’ [doc_1, doc_3, doc_7]
"docker"  â†’ [doc_2, doc_8]
...

RequÃªte: "wifi passe"
â†’ Intersection: [doc_1] âœ…
```

### 10.4 Migration SQL pour Hybrid Search

```sql
-- src/migrations/006_add_fulltext_search.sql

-- 1. Ajouter la colonne tsvector
ALTER TABLE chunks ADD COLUMN IF NOT EXISTS search_vector tsvector;

-- 2. CrÃ©er l'index GIN
CREATE INDEX IF NOT EXISTS chunks_search_idx ON chunks USING GIN(search_vector);

-- 3. Peupler les chunks existants (franÃ§ais + anglais)
UPDATE chunks
SET search_vector = to_tsvector('french', content) || to_tsvector('english', content)
WHERE search_vector IS NULL;

-- 4. Trigger pour auto-update sur INSERT/UPDATE
CREATE OR REPLACE FUNCTION update_chunks_search_vector()
RETURNS TRIGGER AS $$
BEGIN
  NEW.search_vector := to_tsvector('french', NEW.content)
                    || to_tsvector('english', NEW.content);
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER chunks_search_vector_trigger
  BEFORE INSERT OR UPDATE OF content ON chunks
  FOR EACH ROW
  EXECUTE FUNCTION update_chunks_search_vector();
```

**Pourquoi franÃ§ais ET anglais ?**

- Le stemming varie selon la langue
- "documentation" en franÃ§ais â‰  "documentation" en anglais
- On maximise les chances de match

### 10.5 ImplÃ©mentation Repository

```typescript
// src/infrastructure/persistence/DocumentRepository.ts

public async searchByKeywords(
  query: string,
  limit = 10
): Promise<ChunkWithRank[]> {
  const results = await this.prisma.$queryRawUnsafe<{
    id: number;
    document_id: number;
    document_title: string | null;
    content: string;
    chunk_index: number;
    rank: number;
  }[]>(
    `SELECT c.id, c.document_id, d.title as document_title,
            c.content, c.chunk_index,
            ts_rank(
              c.search_vector,
              plainto_tsquery('french', $1) || plainto_tsquery('english', $1)
            ) as rank
     FROM chunks c
     JOIN documents d ON c.document_id = d.id
     WHERE c.search_vector @@ (
       plainto_tsquery('french', $1) || plainto_tsquery('english', $1)
     )
     ORDER BY rank DESC
     LIMIT $2`,
    query,
    limit
  );

  return results.map((r) => ({
    id: Number(r.id),
    documentId: Number(r.document_id),
    documentTitle: r.document_title,
    content: r.content,
    chunkIndex: r.chunk_index,
    rank: Number(r.rank),
  }));
}
```

### 10.6 RRF - Reciprocal Rank Fusion

L'algorithme **RRF** (Reciprocal Rank Fusion) combine les rÃ©sultats de plusieurs sources de ranking.

#### 10.6.1 Le problÃ¨me de la fusion

Comment combiner deux rankings diffÃ©rents ?

```
Vector Search:          Keyword Search:
1. Doc A (dist: 0.2)    1. Doc C (rank: 0.95)
2. Doc B (dist: 0.3)    2. Doc A (rank: 0.80)
3. Doc C (dist: 0.4)    3. Doc D (rank: 0.75)
4. Doc D (dist: 0.5)    4. Doc B (rank: 0.60)

ProblÃ¨me : Les scores ne sont pas comparables !
- Distance cosinus : 0-2 (plus bas = mieux)
- ts_rank : 0-1 (plus haut = mieux)
```

#### 10.6.2 La solution RRF

RRF ignore les scores bruts et utilise uniquement les **positions** (rangs) :

```
Score RRF = Î£ (1 / (k + rang))

OÃ¹ k est une constante (typiquement 60)
```

**Pourquoi k=60 ?**

- k trop petit : les premiers rangs dominent trop
- k trop grand : les diffÃ©rences de rang sont aplaties
- k=60 est empiriquement un bon compromis

#### 10.6.3 Exemple de calcul

```
k = 60

Doc A: rang_vector=1, rang_keyword=2
Score_A = 1/(60+1) + 1/(60+2) = 0.0164 + 0.0161 = 0.0325

Doc B: rang_vector=2, rang_keyword=4
Score_B = 1/(60+2) + 1/(60+4) = 0.0161 + 0.0156 = 0.0317

Doc C: rang_vector=3, rang_keyword=1
Score_C = 1/(60+3) + 1/(60+1) = 0.0159 + 0.0164 = 0.0323

Doc D: rang_vector=4, rang_keyword=3
Score_D = 1/(60+4) + 1/(60+3) = 0.0156 + 0.0159 = 0.0315

Ranking final: A > C > B > D
```

**Observation** : Doc A gagne car il est bien classÃ© dans LES DEUX rankings.

#### 10.6.4 ImplÃ©mentation

```typescript
// src/application/services/rag/HybridSearchService.ts

export class HybridSearchService {
  private readonly documentService: IDocumentService;
  private readonly RRF_K = 60;

  async search(
    query: string,
    options: HybridSearchOptions = {}
  ): Promise<HybridSearchResult[]> {
    // 1. ExÃ©cuter les deux recherches en parallÃ¨le
    const [vectorResults, keywordResults] = await Promise.all([
      this.documentService.searchByQuery(query, { limit: options.limit * 2 }),
      this.documentService.searchByKeywords(query, options.limit * 2),
    ]);

    // 2. Fusionner avec RRF
    return this.fuseWithRRF(vectorResults, keywordResults, options.limit);
  }

  private fuseWithRRF(
    vectorResults: ChunkWithDistance[],
    keywordResults: ChunkWithRank[],
    limit: number
  ): HybridSearchResult[] {
    const scoreMap = new Map<number, HybridSearchResult>();

    // Scores des rÃ©sultats vectoriels
    vectorResults.forEach((chunk, index) => {
      const vectorRank = index + 1;
      const rrfScore = 1 / (this.RRF_K + vectorRank);

      scoreMap.set(chunk.id, {
        id: chunk.id,
        documentId: chunk.documentId,
        documentTitle: chunk.documentTitle,
        content: chunk.content,
        vectorRank,
        rrfScore,
        distance: chunk.distance,
      });
    });

    // Ajouter les scores keyword
    keywordResults.forEach((chunk, index) => {
      const keywordRank = index + 1;
      const keywordScore = 1 / (this.RRF_K + keywordRank);

      const existing = scoreMap.get(chunk.id);
      if (existing) {
        // Document trouvÃ© par les deux mÃ©thodes â†’ boost !
        existing.keywordRank = keywordRank;
        existing.rrfScore += keywordScore;
      } else {
        // TrouvÃ© uniquement par keyword
        scoreMap.set(chunk.id, {
          id: chunk.id,
          documentId: chunk.documentId,
          documentTitle: chunk.documentTitle,
          content: chunk.content,
          keywordRank,
          rrfScore: keywordScore,
        });
      }
    });

    // Trier par score RRF dÃ©croissant
    return Array.from(scoreMap.values())
      .sort((a, b) => b.rrfScore - a.rrfScore)
      .slice(0, limit);
  }
}
```

### 10.7 IntÃ©gration dans le RAG

```typescript
// src/application/services/rag/RAGService.ts

async buildEnrichedPrompt(
  userMessage: string,
  options: RAGOptions = {}
): Promise<RAGContext> {
  // Query rewriting...
  const searchQuery = await this.rewriteQueryIfEnabled(userMessage, options);

  // Hybrid Search activÃ© ?
  if (this.shouldUseHybridSearch(options)) {
    return this.performHybridSearch(searchQuery, options);
  }

  // Sinon : recherche vectorielle classique
  return this.performVectorSearch(searchQuery, options);
}

private async performHybridSearch(
  searchQuery: string,
  options: RAGOptions
): Promise<RAGContext> {
  // Recherche hybride
  const hybridResults = await this.hybridSearchService.search(searchQuery, {
    limit: this.config.rerankCandidates,
    maxDistance: this.config.maxDistance,
  });

  if (hybridResults.length === 0) {
    return this.emptyContext();
  }

  this.logger.info(`ğŸ” Hybrid search: ${hybridResults.length} results`);

  // Convertir en ChunkWithDistance pour compatibilitÃ©
  const chunks = this.hybridResultsToChunks(hybridResults);

  // Reranking optionnel
  if (this.shouldUseReranking(options)) {
    const { chunks: rerankedChunks, sources } =
      await this.rerankChunks(searchQuery, chunks);
    return this.buildContextFromChunks(rerankedChunks, sources);
  }

  return this.buildContextFromChunks(chunks, this.buildHybridSources(hybridResults));
}
```

### 10.8 Configuration et Toggle Frontend

**Backend** :

```typescript
// RAGConfig
export interface RAGConfig {
  maxDocuments: number; // 3
  maxDistance: number; // 0.8
  useReranking: boolean; // true
  useHybridSearch: boolean; // false (opt-in)
  rerankCandidates: number; // 10
}

// RAGOptions (par requÃªte)
export interface RAGOptions {
  useReranking?: boolean;
  useQueryRewrite?: boolean;
  useHybridSearch?: boolean; // â† Nouveau
  conversationHistory?: string[];
}
```

**Frontend** :

```vue
<div class="options-row">
  <label class="rag-toggle">
    <input type="checkbox" v-model="useRAG" />
    <span>ğŸ“š RAG</span>
  </label>
  <label class="rag-toggle" :class="{ disabled: !useRAG }">
    <input type="checkbox" v-model="useQueryRewrite" :disabled="!useRAG" />
    <span>âœï¸ Rewrite</span>
  </label>
  <label class="rag-toggle" :class="{ disabled: !useRAG }">
    <input type="checkbox" v-model="useReranking" :disabled="!useRAG" />
    <span>ğŸ”„ Rerank</span>
  </label>
  <label class="rag-toggle" :class="{ disabled: !useRAG }">
    <input type="checkbox" v-model="useHybridSearch" :disabled="!useRAG" />
    <span>ğŸ” Hybrid</span>
  </label>
</div>
```

### 10.9 Comparaison des mÃ©thodes de recherche

| CritÃ¨re             | Vector seul  | Keyword seul | Hybrid       |
| ------------------- | ------------ | ------------ | ------------ |
| **Sens/Synonymes**  | âœ… Excellent | âŒ Aucun     | âœ… Excellent |
| **Mots exacts**     | âš ï¸ Variable  | âœ… Excellent | âœ… Excellent |
| **Codes/Acronymes** | âŒ Faible    | âœ… Excellent | âœ… Excellent |
| **Noms propres**    | âŒ Faible    | âœ… Excellent | âœ… Excellent |
| **Latence**         | ~50ms        | ~20ms        | ~70ms        |
| **ComplexitÃ©**      | Moyenne      | Faible       | Haute        |

### 10.10 Performance et optimisation

#### Latence

| Ã‰tape                 | Temps      | Optimisation                   |
| --------------------- | ---------- | ------------------------------ |
| GÃ©nÃ©ration embedding  | ~100ms     | Cache embeddings frÃ©quents     |
| Recherche vectorielle | ~30ms      | Index HNSW                     |
| Recherche full-text   | ~20ms      | Index GIN                      |
| Fusion RRF            | ~1ms       | En mÃ©moire                     |
| **Total hybrid**      | **~150ms** | ParallÃ©lisation des recherches |

#### ParallÃ©lisation

```typescript
// Les deux recherches sont indÃ©pendantes â†’ Promise.all
const [vectorResults, keywordResults] = await Promise.all([
  this.documentService.searchByQuery(query, options),
  this.documentService.searchByKeywords(query, limit),
]);
```

### 10.11 Quand utiliser Hybrid Search ?

**âœ… RecommandÃ© pour** :

- Bases de connaissances avec codes/rÃ©fÃ©rences techniques
- Documents contenant des noms propres (personnes, produits)
- RequÃªtes potentiellement contenant des acronymes
- Quand la prÃ©cision est plus importante que la latence

**âŒ Pas nÃ©cessaire pour** :

- RequÃªtes purement conversationnelles
- Petits corpus (<100 documents)
- Quand la latence est critique
- Documents homogÃ¨nes sans termes techniques

### 10.12 SchÃ©ma de la base de donnÃ©es

```sql
-- Table chunks avec support hybrid search
CREATE TABLE chunks (
  id BIGSERIAL PRIMARY KEY,
  document_id BIGINT NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
  content TEXT NOT NULL,
  embedding vector(1024) NOT NULL,     -- Pour vector search
  search_vector tsvector,               -- Pour keyword search
  chunk_index INTEGER NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Index pour vector search (HNSW ou IVFFlat)
CREATE INDEX chunks_embedding_idx ON chunks
USING hnsw (embedding vector_cosine_ops);

-- Index pour keyword search (GIN)
CREATE INDEX chunks_search_idx ON chunks
USING GIN(search_vector);
```

### 10.13 Points clÃ©s Ã  retenir

| Concept             | Explication                                             |
| ------------------- | ------------------------------------------------------- |
| **tsvector**        | ReprÃ©sentation tokenisÃ©e d'un texte pour recherche      |
| **tsquery**         | RequÃªte de recherche textuelle                          |
| **GIN index**       | Index inversÃ© pour recherche full-text rapide           |
| **RRF**             | Algorithme de fusion basÃ© sur les rangs, pas les scores |
| **k=60**            | Constante RRF pour Ã©quilibrer les rangs                 |
| **ParallÃ©lisation** | Les deux recherches sont indÃ©pendantes                  |
| **Trigger**         | Auto-update du search_vector Ã  l'insertion              |

---

## 11. Architecture Clean Architecture

Le projet utilise une **Clean Architecture** (aussi appelÃ©e Hexagonal Architecture ou Ports & Adapters) pour une meilleure sÃ©paration des responsabilitÃ©s et testabilitÃ©.

### 11.1 Les couches

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INFRASTRUCTURE                            â”‚
â”‚  (HTTP, Base de donnÃ©es, APIs externes)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                        APPLICATION                         â”‚  â”‚
â”‚  â”‚  (Use Cases, Services, Ports)                             â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚                      DOMAIN                          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  (EntitÃ©s, Value Objects, RÃ¨gles mÃ©tier)            â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**RÃ¨gle de dÃ©pendance** : Les couches internes ne connaissent pas les couches externes.

### 11.2 Structure des fichiers

```
src/
â”œâ”€â”€ domain/                          # ğŸŸ¢ COUCHE DOMAINE (cÅ“ur mÃ©tier)
â”‚   â”œâ”€â”€ conversation/
â”‚   â”‚   â”œâ”€â”€ entities/
â”‚   â”‚   â”‚   â”œâ”€â”€ Conversation.ts      # EntitÃ© Conversation
â”‚   â”‚   â”‚   â””â”€â”€ Message.ts           # EntitÃ© Message
â”‚   â”‚   â”œâ”€â”€ errors/
â”‚   â”‚   â”‚   â””â”€â”€ ConversationErrors.ts
â”‚   â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â”‚   â””â”€â”€ IConversationRepository.ts  # Interface (port)
â”‚   â”‚   â””â”€â”€ valueObjects/
â”‚   â”‚       â””â”€â”€ MessageRole.ts
â”‚   â”œâ”€â”€ document/
â”‚   â”‚   â”œâ”€â”€ entities/
â”‚   â”‚   â”‚   â””â”€â”€ Document.ts
â”‚   â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â”‚   â””â”€â”€ IDocumentRepository.ts
â”‚   â”‚   â””â”€â”€ valueObjects/
â”‚   â”‚       â””â”€â”€ Embedding.ts
â”‚   â””â”€â”€ shared/
â”‚       â”œâ”€â”€ errors/
â”‚       â”‚   â””â”€â”€ DomainError.ts
â”‚       â””â”€â”€ valueObjects/
â”‚           â””â”€â”€ UUID.ts
â”‚
â”œâ”€â”€ application/                     # ğŸ”µ COUCHE APPLICATION (orchestration)
â”‚   â”œâ”€â”€ ports/
â”‚   â”‚   â”œâ”€â”€ in/                      # Ports d'entrÃ©e (ce que l'app expose)
â”‚   â”‚   â”‚   â”œâ”€â”€ conversation.ts      # Interfaces des use cases
â”‚   â”‚   â”‚   â””â”€â”€ document.ts
â”‚   â”‚   â””â”€â”€ out/                     # Ports de sortie (ce dont l'app a besoin)
â”‚   â”‚       â”œâ”€â”€ IConversationService.ts
â”‚   â”‚       â”œâ”€â”€ IDocumentService.ts
â”‚   â”‚       â”œâ”€â”€ IMistralClient.ts
â”‚   â”‚       â””â”€â”€ IRAGService.ts
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ conversation/
â”‚   â”‚   â”‚   â””â”€â”€ ConversationService.ts
â”‚   â”‚   â”œâ”€â”€ document/
â”‚   â”‚   â”‚   â””â”€â”€ DocumentService.ts
â”‚   â”‚   â””â”€â”€ rag/
â”‚   â”‚       â””â”€â”€ RAGService.ts        # Service RAG
â”‚   â””â”€â”€ usecases/
â”‚       â”œâ”€â”€ conversation/
â”‚       â”‚   â”œâ”€â”€ CreateConversationUseCase.ts
â”‚       â”‚   â”œâ”€â”€ SendMessageUseCase.ts
â”‚       â”‚   â””â”€â”€ StreamMessageUseCase.ts  # Use case principal du chat
â”‚       â””â”€â”€ document/
â”‚           â”œâ”€â”€ AddDocumentUseCase.ts
â”‚           â”œâ”€â”€ SearchDocumentsUseCase.ts
â”‚           â””â”€â”€ ...
â”‚
â”œâ”€â”€ infrastructure/                  # ğŸ”´ COUCHE INFRASTRUCTURE (dÃ©tails techniques)
â”‚   â”œâ”€â”€ http/
â”‚   â”‚   â”œâ”€â”€ controllers/
â”‚   â”‚   â”‚   â”œâ”€â”€ conversationController.ts
â”‚   â”‚   â”‚   â””â”€â”€ documentController.ts
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ conversationRoutes.ts
â”‚   â”‚   â”‚   â””â”€â”€ documentRoutes.ts
â”‚   â”‚   â””â”€â”€ middlewares/
â”‚   â”‚       â””â”€â”€ errorHandler.ts
â”‚   â”œâ”€â”€ persistence/
â”‚   â”‚   â”œâ”€â”€ ConversationRepository.ts  # ImplÃ©mente IConversationRepository
â”‚   â”‚   â””â”€â”€ DocumentRepository.ts      # ImplÃ©mente IDocumentRepository
â”‚   â”œâ”€â”€ external/
â”‚   â”‚   â”œâ”€â”€ mistral/
â”‚   â”‚   â”‚   â”œâ”€â”€ MistralClient.ts       # ImplÃ©mente IMistralClient
â”‚   â”‚   â”‚   â”œâ”€â”€ tokenizer.ts
â”‚   â”‚   â”‚   â””â”€â”€ errors.ts
â”‚   â”‚   â””â”€â”€ rerank/
â”‚   â”‚       â”œâ”€â”€ RerankClient.ts        # Client HTTP pour le service Python
â”‚   â”‚       â”œâ”€â”€ types.ts
â”‚   â”‚       â””â”€â”€ errors.ts
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â””â”€â”€ retry.ts                   # Exponential backoff
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ prisma.ts
â”‚
â”œâ”€â”€ server.ts                        # Point d'entrÃ©e
â”‚
rerank-service/                      # ğŸ SERVICE PYTHON (microservice ML)
â”œâ”€â”€ main.py                          # FastAPI + CrossEncoder
â”œâ”€â”€ requirements.txt                 # DÃ©pendances Python
â””â”€â”€ Dockerfile                       # Image Docker
```

### 11.3 ResponsabilitÃ©s par couche

| Couche             | Contient                                  | ResponsabilitÃ©                                 |
| ------------------ | ----------------------------------------- | ---------------------------------------------- |
| **Domain**         | Entities, Value Objects, Interfaces repos | RÃ¨gles mÃ©tier pures, aucune dÃ©pendance externe |
| **Application**    | Use Cases, Services, Ports                | Orchestrer les cas d'utilisation               |
| **Infrastructure** | Controllers, Repositories, Clients API    | ImplÃ©menter les dÃ©tails techniques             |

### 11.4 Les Ports (Interfaces)

Les **ports** dÃ©finissent des contrats que l'infrastructure doit respecter :

```typescript
// application/ports/out/IMistralClient.ts
export interface IMistralClient {
  chat(message: string, options?: ChatOptions): Promise<string | null>;
  streamComplete(messages: ChatMessage[]): AsyncIterable<string>;
  generateEmbedding(text: string): Promise<number[]>;
}

// infrastructure/external/mistral/MistralClient.ts
export class MistralClient implements IMistralClient {
  // ImplÃ©mentation concrÃ¨te avec le SDK Mistral
}
```

**Avantage** : On peut remplacer `MistralClient` par un mock pour les tests !

### 11.5 Les Use Cases

Un **Use Case** reprÃ©sente une action mÃ©tier unique :

```typescript
// application/usecases/conversation/StreamMessageUseCase.ts

export class StreamMessageUseCase {
  constructor(
    private conversationService: IConversationService,
    private mistralClient: IMistralClient,
    private ragService: IRAGService,  // Injection de dÃ©pendances
  ) {}

  async *execute(input: StreamMessageInput): AsyncGenerator<StreamMessageChunk> {
    // 1. Sauvegarder le message
    await this.conversationService.addMessage(...);

    // 2. RÃ©cupÃ©rer l'historique
    const chatHistory = await this.conversationService.getChatHistory(...);

    // 3. Enrichir avec RAG
    const ragContext = await this.ragService.buildEnrichedPrompt(message);
    chatHistory[0].content = ragContext.enrichedPrompt;

    // 4. Streamer la rÃ©ponse
    for await (const chunk of this.mistralClient.streamComplete(chatHistory)) {
      yield { chunk };
    }
  }
}
```

### 11.6 Flux d'une requÃªte (Clean Architecture)

```
POST /api/chat/stream
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INFRASTRUCTURE: conversationController.ts                       â”‚
â”‚   â†’ Valide la requÃªte HTTP                                      â”‚
â”‚   â†’ Appelle le Use Case                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APPLICATION: StreamMessageUseCase.ts                            â”‚
â”‚   â†’ Orchestre la logique mÃ©tier                                 â”‚
â”‚   â†’ Utilise les Services via leurs interfaces (ports)          â”‚
â”‚   â”‚                                                             â”‚
â”‚   â”œâ”€â”€ ConversationService.addMessage()                          â”‚
â”‚   â”œâ”€â”€ ConversationService.getChatHistory()                      â”‚
â”‚   â”œâ”€â”€ RAGService.buildEnrichedPrompt()  â—„â”€â”€ Recherche docs     â”‚
â”‚   â””â”€â”€ MistralClient.streamComplete()                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INFRASTRUCTURE: ImplÃ©mentations concrÃ¨tes                       â”‚
â”‚   â”‚                                                             â”‚
â”‚   â”œâ”€â”€ ConversationRepository (Prisma + PostgreSQL)              â”‚
â”‚   â”œâ”€â”€ DocumentRepository (pgvector)                             â”‚
â”‚   â””â”€â”€ MistralClient (SDK Mistral)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EXTERNAL: APIs et Base de donnÃ©es                               â”‚
â”‚   â”œâ”€â”€ Mistral AI API                                            â”‚
â”‚   â””â”€â”€ PostgreSQL + pgvector                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 11.7 Avantages de cette architecture

| Avantage           | Description                                   |
| ------------------ | --------------------------------------------- |
| **TestabilitÃ©**    | On peut mocker les dÃ©pendances externes       |
| **MaintenabilitÃ©** | Chaque couche a une responsabilitÃ© claire     |
| **FlexibilitÃ©**    | Changer de DB ou d'API sans toucher au mÃ©tier |
| **IndÃ©pendance**   | Le domaine ne dÃ©pend de rien                  |

---

## 12. Frontend Vue.js

### 12.1 Composants

```
frontend/src/
â”œâ”€â”€ views/
â”‚   â””â”€â”€ ChatBot.vue           # Vue principale
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ChatInput.vue         # Champ de saisie
â”‚   â”œâ”€â”€ ChatMessage.vue       # Bulle de message
â”‚   â””â”€â”€ ChatSidebar.vue       # Liste des conversations
â””â”€â”€ router/
    â””â”€â”€ index.ts              # Vue Router
```

### 12.2 Gestion du streaming

```vue
<script setup>
const messages = ref([]);

async function sendMessage(content) {
  // 1. Ajouter le message user
  messages.value.push({ role: 'user', content });

  // 2. PrÃ©parer le message assistant (vide)
  const assistantIndex = messages.value.length;
  messages.value.push({ role: 'assistant', content: '' });

  // 3. Stream la rÃ©ponse
  const reader = response.body.getReader();
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    // 4. Mettre Ã  jour en temps rÃ©el
    messages.value[assistantIndex].content += chunk;
  }
}
</script>
```

---

## 13. Concepts clÃ©s Ã  retenir

### 13.1 Patterns

| Pattern                  | OÃ¹                           | Pourquoi                               |
| ------------------------ | ---------------------------- | -------------------------------------- |
| **Clean Architecture**   | Structure du projet          | SÃ©paration des responsabilitÃ©s         |
| **Ports & Adapters**     | Interfaces + ImplÃ©mentations | DÃ©couplage, testabilitÃ©                |
| **Use Case**             | StreamMessageUseCase         | Une action mÃ©tier = une classe         |
| **Dependency Injection** | Constructeurs Use Cases      | Injecter les dÃ©pendances (mocks)       |
| **Repository**           | ConversationRepository       | Abstraction de la persistance          |
| **Singleton**            | getMistralClient()           | Une seule instance, config partagÃ©e    |
| **AsyncIterator**        | streamComplete()             | Traiter les donnÃ©es au fur et Ã  mesure |
| **Exponential Backoff**  | withRetry()                  | RÃ©silience aux erreurs API             |
| **Sliding Window**       | tokenizer.ts                 | GÃ©rer les limites de contexte          |
| **RAG**                  | RAGService                   | Enrichir le LLM avec des docs privÃ©s   |
| **Query Rewriting**      | QueryRewriterService         | Optimiser la query avant recherche     |
| **Two-Stage Retrieval**  | RAGService + RerankClient    | Recherche large â†’ reranking prÃ©cis     |
| **Hybrid Search**        | HybridSearchService          | Combiner vector + keyword search       |
| **RRF (Rank Fusion)**    | HybridSearchService          | Fusionner rankings sans scores bruts   |
| **Fallback gracieux**    | RerankClient.isAvailable()   | Continuer si service indisponible      |
| **Microservice**         | rerank-service (Python)      | Isoler le modÃ¨le ML du backend Node.js |
| **Zod Validation**       | schemas/\*.schema.ts         | Validation typÃ©e et rÃ©utilisable       |

### 13.2 Bonnes pratiques

1. **SÃ©paration des responsabilitÃ©s** : Routes â†’ Controllers â†’ Services
2. **Typage fort** : Interfaces TypeScript partout
3. **Gestion des erreurs** : Classes d'erreurs custom
4. **Configuration** : Variables d'environnement, pas de hardcode
5. **Logging** : Console.log pour debug, avec prÃ©fixes `[ServiceName]`
6. **ParamÃ¨tres SQL** : Toujours utiliser `$1, $2` au lieu de concatÃ©nation

### 13.3 Points de vigilance

| ProblÃ¨me                 | Solution                       |
| ------------------------ | ------------------------------ |
| Rate limiting (429)      | Retry avec exponential backoff |
| Context overflow (400)   | Sliding window                 |
| Latence UX               | Streaming SSE                  |
| Perte de contexte        | Persistance PostgreSQL         |
| Erreurs silencieuses     | Classes d'erreurs typÃ©es       |
| Longs documents          | Chunking avant embedding       |
| RÃ©sultats non pertinents | Filtrer par maxDistance        |
| Queries mal formulÃ©es    | Query Rewriting avec LLM       |
| RÃ©sultats mal classÃ©s    | Reranking avec cross-encoder   |
| Service rerank down      | Fallback vers recherche vector |
| Latence reranking        | Limiter les candidats (10 max) |
| Codes/noms non trouvÃ©s   | Hybrid Search (vector+keyword) |
| Validation manuelle      | SchÃ©mas Zod centralisÃ©s        |

### 13.4 Commandes utiles

```bash
# GÃ©nÃ©rer le client Prisma
pnpm prisma:generate

# Pousser le schÃ©ma en DB
pnpm db:push

# Reset la DB
pnpm db:push --force-reset

# Migrations SQL personnalisÃ©es (pour pgvector)
docker compose exec app sh -c "cd /app/src && pnpm migrate"

# Logs Docker
docker compose logs app --tail=20
docker compose logs frontend --tail=20

# Activer pgvector
docker compose exec postgres psql -U postgres -d ia_chat -c "CREATE EXTENSION IF NOT EXISTS vector;"

# Voir les documents indexÃ©s (avec tracking des chunks)
docker compose exec postgres psql -U postgres -d ia_chat -c "SELECT id, LEFT(content, 50), source_id, chunk_index FROM documents;"

# Voir les migrations appliquÃ©es
docker compose exec postgres psql -U postgres -d ia_chat -c "SELECT * FROM _migrations;"

# === Service de Reranking ===

# Construire l'image (tÃ©lÃ©charge le modÃ¨le ~280MB)
docker build --network=host -t ia-project-rerank ./rerank-service

# DÃ©marrer le service
docker compose up -d rerank

# VÃ©rifier la santÃ© du service
curl http://localhost:8001/health

# Logs du service rerank
docker compose logs rerank --tail=20

# Tester le reranking manuellement
curl -X POST http://localhost:8001/rerank \
  -H "Content-Type: application/json" \
  -d '{"query": "test", "documents": [{"id": 1, "content": "doc"}], "top_k": 1}'

# === Hybrid Search (Full-Text) ===

# VÃ©rifier la colonne search_vector
docker compose exec postgres psql -U postgres -d ia_chat -c \
  "SELECT id, LEFT(content, 30), search_vector IS NOT NULL as has_fts FROM chunks LIMIT 5;"

# Tester la recherche full-text
docker compose exec postgres psql -U postgres -d ia_chat -c \
  "SELECT id, ts_rank(search_vector, plainto_tsquery('french', 'wifi')) as rank
   FROM chunks WHERE search_vector @@ plainto_tsquery('french', 'wifi');"

# VÃ©rifier l'index GIN
docker compose exec postgres psql -U postgres -d ia_chat -c \
  "SELECT indexname, indexdef FROM pg_indexes WHERE tablename = 'chunks';"
```

### 13.5 SystÃ¨me de migrations SQL

Prisma ne supporte pas le type `vector` de pgvector, donc on utilise un systÃ¨me de migrations SQL personnalisÃ© :

```
src/migrations/
â”œâ”€â”€ 001_create_documents_table.sql   # Table de base
â”œâ”€â”€ 002_add_chunk_tracking.sql       # Colonnes source_id, chunk_index
â””â”€â”€ migrate.ts                       # Script d'exÃ©cution
```

**Structure de la table `_migrations`** :

| id  | name                           | applied_at |
| --- | ------------------------------ | ---------- |
| 1   | 001_create_documents_table.sql | 2024-12-21 |
| 2   | 002_add_chunk_tracking.sql     | 2024-12-21 |

**CrÃ©er une nouvelle migration** :

```sql
-- src/migrations/003_add_my_feature.sql
ALTER TABLE documents ADD COLUMN IF NOT EXISTS my_column TEXT;
```

Puis exÃ©cuter : `docker compose exec app sh -c "cd /app/src && pnpm migrate"`

### 13.6 Fixtures et Seeding

Pour tester l'application avec des donnÃ©es rÃ©alistes :

```bash
# Voir ce qui serait insÃ©rÃ© (sans exÃ©cuter)
docker compose exec app sh -c "cd /app/src && pnpm seed:dry"

# InsÃ©rer les fixtures (conserve les donnÃ©es existantes)
docker compose exec app sh -c "cd /app/src && pnpm seed"

# Nettoyer et rÃ©insÃ©rer (reset complet)
docker compose exec app sh -c "cd /app/src && pnpm seed:clean"
```

**Structure des fixtures** :

```
src/fixtures/
â”œâ”€â”€ documents.ts   # DonnÃ©es de test (FAQ, procÃ©dures, docs techniques)
â”œâ”€â”€ seed.ts        # Script d'exÃ©cution
â””â”€â”€ index.ts       # Exports
```

**Types de documents inclus** :

| CatÃ©gorie       | Exemples                  | Chunking       |
| --------------- | ------------------------- | -------------- |
| Infos pratiques | WiFi, horaires, contacts  | Non            |
| ProcÃ©dures      | CongÃ©s, notes de frais    | Non            |
| Technique       | Docker, Architecture, API | Oui (3 chunks) |
| FAQ             | Questions frÃ©quentes      | Non            |
| RH              | TÃ©lÃ©travail               | Non            |

**Exemple de fixture** :

```typescript
// src/fixtures/documents.ts
export const documentFixtures: DocumentFixture[] = [
  {
    title: 'WiFi et RÃ©seau',
    content: `Le mot de passe WiFi est SecretWifi2024!...`,
    useChunking: false, // Document court â†’ pas de chunking
  },
  {
    title: 'Guide Docker',
    content: `Guide complet de 800 mots...`,
    useChunking: true, // Document long â†’ chunking
    chunkSize: 400,
    overlap: 80,
  },
];
```

---

## ğŸ“ Checklist de rÃ©vision

### Clean Architecture

- [ ] Je connais les 3 couches : Domain, Application, Infrastructure
- [ ] Je comprends la rÃ¨gle de dÃ©pendance (vers l'intÃ©rieur)
- [ ] Je sais ce qu'est un Port (interface) et pourquoi c'est utile
- [ ] Je sais ce qu'est un Use Case et son rÃ´le
- [ ] Je comprends l'injection de dÃ©pendances

### Patterns

- [ ] Je comprends le pattern Singleton
- [ ] Je sais implÃ©menter un retry avec exponential backoff
- [ ] Je comprends pourquoi le jitter est important
- [ ] Je sais ce qu'est une sliding window et pourquoi c'est nÃ©cessaire

### Streaming & Frontend

- [ ] Je peux expliquer le flux SSE (Server-Sent Events)
- [ ] Je comprends les AsyncIterables (`async *` et `yield`)
- [ ] Je peux consommer un stream cÃ´tÃ© frontend

### Base de donnÃ©es

- [ ] Je sais crÃ©er un schÃ©ma Prisma
- [ ] Je comprends les transactions et niveaux d'isolation
- [ ] Je sais utiliser `$queryRawUnsafe` avec des paramÃ¨tres positionnels
- [ ] Je sais crÃ©er des migrations SQL pour les types non supportÃ©s par Prisma (vector)

### Embeddings & RAG

- [ ] Je comprends ce qu'est un embedding (texte â†’ vecteur)
- [ ] Je sais gÃ©nÃ©rer un embedding avec `mistral-embed`
- [ ] Je comprends la distance cosinus et comment l'interprÃ©ter
- [ ] Je sais stocker des vecteurs avec pgvector
- [ ] Je peux implÃ©menter une recherche sÃ©mantique
- [ ] Je comprends le concept de RAG et son utilitÃ©
- [ ] Je sais que le system prompt est rÃ©Ã©crit Ã  chaque message avec le contexte RAG
- [ ] Je sais pourquoi le chunking est important pour les longs documents
- [ ] Je comprends le chunking avec overlap et pourquoi c'est mieux que sans
- [ ] Je sais calculer le step : `step = chunkSize - overlap`
- [ ] Je connais les algorithmes de recherche vectorielle (Force brute, IVFFlat, HNSW)

### Reranking

- [ ] Je comprends la diffÃ©rence entre Bi-Encoder (embeddings) et Cross-Encoder (reranking)
- [ ] Je sais pourquoi le reranking amÃ©liore la prÃ©cision vs la recherche vectorielle seule
- [ ] Je connais le pattern "two-stage retrieval" (recherche large â†’ reranking prÃ©cis)
- [ ] Je comprends pourquoi on utilise un service Python sÃ©parÃ© (Ã©cosystÃ¨me ML, isolation)
- [ ] Je sais implÃ©menter un fallback gracieux (continuer sans reranking si le service tombe)
- [ ] Je comprends la conversion score â†’ similaritÃ© avec la fonction sigmoid
- [ ] Je sais configurer et dÃ©ployer un service de reranking avec Docker

### Microservices & Architecture distribuÃ©e

- [ ] Je comprends l'intÃ©rÃªt d'isoler les modÃ¨les ML dans des services dÃ©diÃ©s
- [ ] Je sais implÃ©menter un healthcheck pour vÃ©rifier la disponibilitÃ© d'un service
- [ ] Je comprends le cache de disponibilitÃ© (Ã©viter les appels rÃ©pÃ©tÃ©s)
- [ ] Je sais gÃ©rer les erreurs rÃ©seau (timeout, service indisponible)

### Hybrid Search

- [ ] Je comprends la diffÃ©rence entre recherche vectorielle et recherche par mots-clÃ©s
- [ ] Je sais quand utiliser Hybrid Search (codes produits, noms propres, acronymes)
- [ ] Je comprends ce qu'est un `tsvector` et un `tsquery` dans PostgreSQL
- [ ] Je sais crÃ©er un index GIN pour accÃ©lÃ©rer la recherche full-text
- [ ] Je comprends l'algorithme RRF (Reciprocal Rank Fusion)
- [ ] Je sais pourquoi RRF utilise les rangs et non les scores bruts
- [ ] Je comprends le rÃ´le de la constante k=60 dans RRF
- [ ] Je sais parallÃ©liser les recherches vectorielle et keyword avec Promise.all
- [ ] Je peux crÃ©er un trigger PostgreSQL pour auto-update de colonnes

### Validation (Zod)

- [ ] Je comprends les avantages de Zod vs validation manuelle
- [ ] Je sais crÃ©er des schÃ©mas de validation avec Zod
- [ ] Je comprends l'infÃ©rence de types avec `z.infer<typeof Schema>`
- [ ] Je sais utiliser `.refine()` pour des validations cross-field
- [ ] Je peux crÃ©er des helpers de validation rÃ©utilisables

---

_Document mis Ã  jour le 22/12/2024_
